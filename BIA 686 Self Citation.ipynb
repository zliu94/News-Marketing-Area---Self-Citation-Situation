{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PT                                                 AU  BA  BE  GP  \\\n",
      "0   J                             Hanusch, F; Tandoc, EC NaN NaN NaN   \n",
      "0   J                             Hanusch, F; Tandoc, EC NaN NaN NaN   \n",
      "1   J                           Seok, SI; Cho, H; Ryu, D NaN NaN NaN   \n",
      "1   J                           Seok, SI; Cho, H; Ryu, D NaN NaN NaN   \n",
      "1   J                           Seok, SI; Cho, H; Ryu, D NaN NaN NaN   \n",
      ".. ..                                                ...  ..  ..  ..   \n",
      "42  J                          Dellavigna, S; Pollet, JM NaN NaN NaN   \n",
      "43  J  Curran, J; Iyengar, S; Lund, AB; Salovaara-Mor... NaN NaN NaN   \n",
      "43  J  Curran, J; Iyengar, S; Lund, AB; Salovaara-Mor... NaN NaN NaN   \n",
      "43  J  Curran, J; Iyengar, S; Lund, AB; Salovaara-Mor... NaN NaN NaN   \n",
      "43  J  Curran, J; Iyengar, S; Lund, AB; Salovaara-Mor... NaN NaN NaN   \n",
      "\n",
      "                                                   AF  BF  CA  \\\n",
      "0              Hanusch, Folker; Tandoc, Edson C., Jr. NaN NaN   \n",
      "0              Hanusch, Folker; Tandoc, Edson C., Jr. NaN NaN   \n",
      "1               Seok, Sang Ik; Cho, Hoon; Ryu, Doojin NaN NaN   \n",
      "1               Seok, Sang Ik; Cho, Hoon; Ryu, Doojin NaN NaN   \n",
      "1               Seok, Sang Ik; Cho, Hoon; Ryu, Doojin NaN NaN   \n",
      "..                                                ...  ..  ..   \n",
      "42             Dellavigna, Stefano; Pollet, Joshua M. NaN NaN   \n",
      "43  Curran, James; Iyengar, Shanto; Lund, Anker Br... NaN NaN   \n",
      "43  Curran, James; Iyengar, Shanto; Lund, Anker Br... NaN NaN   \n",
      "43  Curran, James; Iyengar, Shanto; Lund, Anker Br... NaN NaN   \n",
      "43  Curran, James; Iyengar, Shanto; Lund, Anker Br... NaN NaN   \n",
      "\n",
      "                                                   TI  \\\n",
      "0   Comments, analytics, and social media: The imp...   \n",
      "0   Comments, analytics, and social media: The imp...   \n",
      "1   Firm-specific investor sentiment and the stock...   \n",
      "1   Firm-specific investor sentiment and the stock...   \n",
      "1   Firm-specific investor sentiment and the stock...   \n",
      "..                                                ...   \n",
      "42  Investor Inattention and Friday Earnings Annou...   \n",
      "43  Media System, Public Knowledge and Democracy A...   \n",
      "43  Media System, Public Knowledge and Democracy A...   \n",
      "43  Media System, Public Knowledge and Democracy A...   \n",
      "43  Media System, Public Knowledge and Democracy A...   \n",
      "\n",
      "                                                 SO  ...  \\\n",
      "0                                        JOURNALISM  ...   \n",
      "0                                        JOURNALISM  ...   \n",
      "1   NORTH AMERICAN JOURNAL OF ECONOMICS AND FINANCE  ...   \n",
      "1   NORTH AMERICAN JOURNAL OF ECONOMICS AND FINANCE  ...   \n",
      "1   NORTH AMERICAN JOURNAL OF ECONOMICS AND FINANCE  ...   \n",
      "..                                              ...  ...   \n",
      "42                               JOURNAL OF FINANCE  ...   \n",
      "43                EUROPEAN JOURNAL OF COMMUNICATION  ...   \n",
      "43                EUROPEAN JOURNAL OF COMMUNICATION  ...   \n",
      "43                EUROPEAN JOURNAL OF COMMUNICATION  ...   \n",
      "43                EUROPEAN JOURNAL OF COMMUNICATION  ...   \n",
      "\n",
      "                      SC     GA                   UT  PM   OA HC HP  \\\n",
      "0          Communication  HZ7BO  WOS:000469006700001 NaN  NaN  Y  N   \n",
      "0          Communication  HZ7BO  WOS:000469006700001 NaN  NaN  Y  N   \n",
      "1   Business & Economics  HU9WY  WOS:000465646200014 NaN  NaN  Y  N   \n",
      "1   Business & Economics  HU9WY  WOS:000465646200014 NaN  NaN  Y  N   \n",
      "1   Business & Economics  HU9WY  WOS:000465646200014 NaN  NaN  Y  N   \n",
      "..                   ...    ...                  ...  ..  ... .. ..   \n",
      "42  Business & Economics  418YQ  WOS:000264186600005 NaN  NaN  Y  N   \n",
      "43         Communication  421SA  WOS:000264377400001 NaN  NaN  Y  N   \n",
      "43         Communication  421SA  WOS:000264377400001 NaN  NaN  Y  N   \n",
      "43         Communication  421SA  WOS:000264377400001 NaN  NaN  Y  N   \n",
      "43         Communication  421SA  WOS:000264377400001 NaN  NaN  Y  N   \n",
      "\n",
      "            DA ind                  AUN  \n",
      "0   2020-03-04   0           Hanusch, F  \n",
      "0   2020-03-04   0           Tandoc, EC  \n",
      "1   2020-03-04   1             Seok, SI  \n",
      "1   2020-03-04   1               Cho, H  \n",
      "1   2020-03-04   1               Ryu, D  \n",
      "..         ...  ..                  ...  \n",
      "42  2020-03-04  42           Pollet, JM  \n",
      "43  2020-03-04  43            Curran, J  \n",
      "43  2020-03-04  43           Iyengar, S  \n",
      "43  2020-03-04  43             Lund, AB  \n",
      "43  2020-03-04  43  Salovaara-Moring, I  \n",
      "\n",
      "[117 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "news = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\news marketing.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "AUN = news[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AUN = AUN.reset_index(level = 1, drop = True).rename(\"AUN\")\n",
    "news[\"ind\"] = range(len(news))\n",
    "news = news.join(AUN)\n",
    "print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Comments, analytics, and social media The impact of audience feedback on journalists' market orientation.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df0[\"ind\"] = 0\n",
    "AU0 = df0[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU0 = AU0.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df0 = df0.join(AU0)\n",
    "df1 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Firm-specific investor sentiment and the stock market response to earnings news.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df1[\"ind\"] = 1\n",
    "AU1 = df1[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU1 = AU1.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df1 = df1.join(AU1)\n",
    "df2 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Medical Marketing in the United States, 1997-2016.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df2[\"ind\"] = 2\n",
    "AU2 = df2[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU2 = AU2.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df2 = df2.join(AU2)\n",
    "df3 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Advertising Content and Consumer Engagement on Social Media Evidence from Facebook.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df3[\"ind\"] = 3\n",
    "AU3 = df3[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU3 = AU3.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df3 = df3.join(AU3)\n",
    "df4 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Automated Text Analysis for Consumer Research.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df4[\"ind\"] = 4\n",
    "AU4 = df4[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU4 = AU4.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df4 = df4.join(AU4)\n",
    "df5 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Getting the Word Out New Approaches for Disseminating Public Health Science.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df5[\"ind\"] = 5\n",
    "AU5 = df5[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU5 = AU5.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df5 = df5.join(AU5)\n",
    "df6 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Additive manufacturing scientific and technological challenges, market uptake and opportunities.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df6[\"ind\"] = 6\n",
    "AU6 = df6[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU6 = AU6.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df6 = df6.join(AU6)\n",
    "df7 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Price clustering in Bitcoin.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df7[\"ind\"] = 7\n",
    "AU7 = df7[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU7 = AU7.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df7 = df7.join(AU7)\n",
    "df8 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\TECHNOLOGICAL INNOVATION, RESOURCE ALLOCATION, AND GROWTH.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df8[\"ind\"] = 8\n",
    "AU8 = df8[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU8 = AU8.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df8 = df8.join(AU8)\n",
    "df9 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Non-social features of smartphone use are most related to depression, anxiety and problematic smartp.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df9[\"ind\"] = 9\n",
    "AU9 = df9[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU9 = AU9.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df9 = df9.join(AU9)\n",
    "df10 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\News implied volatility and disaster concerns.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df10[\"ind\"] = 10\n",
    "AU10 = df10[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU10 = AU10.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df10 = df10.join(AU10)\n",
    "df11 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Asymmetric impact of gold, oil prices and their volatilities on stock prices of emerging markets.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df11[\"ind\"] = 11\n",
    "AU11 = df11[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU11 = AU11.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df11 = df11.join(AU11)\n",
    "df12 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\News from the seabed - Geological characteristics and resource potential of deep-sea mineral resourc.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df12[\"ind\"] = 12\n",
    "AU12 = df12[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU12 = AU12.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df12 = df12.join(AU12)\n",
    "df13 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Additive manufacturing technologies state of the art and trends.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df13[\"ind\"] = 13\n",
    "AU13 = df13[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU13 = AU13.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df13 = df13.join(AU13)\n",
    "df14 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Brand Buzz in the Echoverse.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df14[\"ind\"] = 14\n",
    "AU14 = df14[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU14 = AU14.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df14 = df14.join(AU14)\n",
    "df15 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Accounting Conservatism and Stock Price Crash Risk Firm-level Evidence.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df15[\"ind\"] = 15\n",
    "AU15 = df15[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU15 = AU15.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df15 = df15.join(AU15)\n",
    "df16 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Policy Uncertainty and Corporate Investment.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df16[\"ind\"] = 16\n",
    "AU16 = df16[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU16 = AU16.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df16 = df16.join(AU16)\n",
    "df17 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Unconventional monetary policy had large international effects.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df17[\"ind\"] = 17\n",
    "AU17 = df17[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU17 = AU17.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df17 = df17.join(AU17)\n",
    "df18 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Corporate goodness and shareholder wealth.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df18[\"ind\"] = 18\n",
    "AU18 = df18[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU18 = AU18.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df18 = df18.join(AU18)\n",
    "df19 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Corporate Investment and Stock Market Listing A Puzzle.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df19[\"ind\"] = 19\n",
    "AU19 = df19[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU19 = AU19.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df19 = df19.join(AU19)\n",
    "df20 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Systematic inequality and hierarchy in faculty hiring networks.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df20[\"ind\"] = 20\n",
    "AU20 = df20[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU20 = AU20.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df20 = df20.join(AU20)\n",
    "df21 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\The Asset-Pricing Implications of Government Economic Policy Uncertainty.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df21[\"ind\"] = 21\n",
    "AU21 = df21[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU21 = AU21.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df21 = df21.join(AU21)\n",
    "df22 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Word of mouth and interpersonal communication A review and directions for future research.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df22[\"ind\"] = 22\n",
    "AU22 = df22[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU22 = AU22.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df22 = df22.join(AU22)\n",
    "df23 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\High-Frequency Trading and Price Discovery.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df23[\"ind\"] = 23\n",
    "AU23 = df23[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU23 = AU23.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df23 = df23.join(AU23)\n",
    "df24 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\The impact of wind power generation on the electricity price in Germany.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df24[\"ind\"] = 24\n",
    "AU24 = df24[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU24 = AU24.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df24 = df24.join(AU24)\n",
    "df25 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Corporate social responsibility and stock price crash risk.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df25[\"ind\"] = 25\n",
    "AU25 = df25[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU25 = AU25.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df25 = df25.join(AU25)\n",
    "df26 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Fostering Consumer-Brand Relationships in Social Media Environments The Role of Parasocial Interact.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df26[\"ind\"] = 26\n",
    "AU26 = df26[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU26 = AU26.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df26 = df26.join(AU26)\n",
    "df27 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\The ironic effects of weight stigma.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df27[\"ind\"] = 27\n",
    "AU27 = df27[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU27 = AU27.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df27 = df27.join(AU27)\n",
    "df28 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Political uncertainty and risk premia.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df28[\"ind\"] = 28\n",
    "AU28 = df28[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU28 = AU28.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df28 = df28.join(AU28)\n",
    "df29 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Glycerol Production, consumption, prices, characterization and new trends in combustion.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df29[\"ind\"] = 29\n",
    "AU29 = df29[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU29 = AU29.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df29 = df29.join(AU29)\n",
    "df30 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\PERSONALIZED CAMPAIGNS IN PARTY-CENTRED POLITICS Twitter and Facebook as arenas for political commu.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df30[\"ind\"] = 30\n",
    "AU30 = df30[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU30 = AU30.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df30 = df30.join(AU30)\n",
    "df31 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\CORPORATE SOCIAL RESPONSIBILITY AND SHAREHOLDER REACTION THE ENVIRONMENTAL AWARENESS OF INVESTORS.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df31[\"ind\"] = 31\n",
    "AU31 = df31[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU31 = AU31.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df31 = df31.join(AU31)\n",
    "df32 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\What Makes Online Content Viral.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df32[\"ind\"] = 32\n",
    "AU32 = df32[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU32 = AU32.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df32 = df32.join(AU32)\n",
    "df33 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\SHARE, LIKE, RECOMMEND Decoding the social media news consumer.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df33[\"ind\"] = 33\n",
    "AU33 = df33[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU33 = AU33.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df33 = df33.join(AU33)\n",
    "df34 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Ideological Segregation Online and Offline.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df34[\"ind\"] = 34\n",
    "AU34 = df34[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU34 = AU34.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df34 = df34.join(AU34)\n",
    "df35 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Corporate tax avoidance and stock price crash risk Firm-level analysis.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df35[\"ind\"] = 35\n",
    "AU35 = df35[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU35 = AU35.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df35 = df35.join(AU35)\n",
    "df36 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\The effect of perceived trust on electronic commerce Shopping online for tourism products and servi.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df36[\"ind\"] = 36\n",
    "AU36 = df36[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU36 = AU36.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df36 = df36.join(AU36)\n",
    "df37 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\How Large and Long-lasting Are the Persuasive Effects of Televised Campaign Ads Results from a Rand.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df37[\"ind\"] = 37\n",
    "AU37 = df37[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU37 = AU37.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df37 = df37.join(AU37)\n",
    "df38 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\The Causal Impact of Media in Financial Markets.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df38[\"ind\"] = 38\n",
    "AU38 = df38[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU38 = AU38.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df38 = df38.join(AU38)\n",
    "df39 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\What Drives Media Slant Evidence From US Daily Newspapers.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df39[\"ind\"] = 39\n",
    "AU39 = df39[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU39 = AU39.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df39 = df39.join(AU39)\n",
    "df40 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Media Coverage and the Cross-section of Stock Returns.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df40[\"ind\"] = 40\n",
    "AU40 = df40[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU40 = AU40.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df40 = df40.join(AU40)\n",
    "df41 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Driven to Distraction Extraneous Events and Underreaction to Earnings News.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df41[\"ind\"] = 41\n",
    "AU41 = df41[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU41 = AU41.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df41 = df41.join(AU41)\n",
    "df42 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Investor Inattention and Friday Earnings Announcements.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df42[\"ind\"] = 42\n",
    "AU42 = df42[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU42 = AU42.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df42 = df42.join(AU42)\n",
    "df43 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\Media System, Public Knowledge and Democracy A Comparative Study.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "df43[\"ind\"] = 43\n",
    "AU43 = df43[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AU43 = AU43.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "df43 = df43.join(AU43)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames = [df0, df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21, df22, df23, df24, df25, df26, df27, df28, df29, df30, df31, df32, df33, df34, df35, df36, df37, df38, df39, df40, df41, df42, df43]\n",
    "alldf = pd.concat(frames, sort = False)\n",
    "# print(alldf)\n",
    "alldf2 = pd.merge(news, alldf, how = \"left\", on = \"ind\")\n",
    "alldf3 = alldf2[[\"AUN\", \"AUC\"]]\n",
    "#print(alldf3.head)\n",
    "#alldf3.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\reviews.csv\", index = False)\n",
    "#alldf2.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\allcolumn.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Seeing Eye to Eye: A Comparison of Audiences' ...\n",
      "1      Role Orientations and Audience Metrics in News...\n",
      "2      Facebook News Use During the 2017 Norwegian El...\n",
      "3      Beaten by Chartbeat? An Experimental Study on ...\n",
      "4      Value Emergence in the Usage of Mobile News Al...\n",
      "                             ...                        \n",
      "156    News sites' position in the mediascape: uses, ...\n",
      "157    The Paradoxes of Media Globalization: On the B...\n",
      "158    THE SHIFTING CROSS-MEDIA NEWS LANDSCAPE Challe...\n",
      "159                                    CHATTING THE NEWS\n",
      "160    The institutional logics of journalism: studyi...\n",
      "Name: TI, Length: 5155, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print(alldf2.head)\n",
    "text_y = alldf[\"TI\"]\n",
    "text_yd = text_y.drop_duplicates(keep='first', inplace=False)\n",
    "# print(len(text_yd))\n",
    "print(text_yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = text_yd.tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words+=[\"cook\", \"cooking\",\"cookbook\",\"cooker\",\"book\",\"recipes\",\"food\", \"doi\"]\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "# print(sorted_dict)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\citing.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "news1 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Web of Science\\\\news marketing.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "title = news1[\"TI\"]\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words+=[\"cook\", \"cooking\",\"cookbook\",\"cooker\",\"book\",\"recipes\",\"food\"]\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "# print(sorted_dict)\n",
    "sorted_dict2 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict2.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\cited.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    TI      PY\n",
      "0    Seeing Eye to Eye: A Comparison of Audiences' ...     NaN\n",
      "1    Role Orientations and Audience Metrics in News...     NaN\n",
      "2    Facebook News Use During the 2017 Norwegian El...     NaN\n",
      "3    Beaten by Chartbeat? An Experimental Study on ...  2020.0\n",
      "4    Value Emergence in the Usage of Mobile News Al...     NaN\n",
      "..                                                 ...     ...\n",
      "156  News sites' position in the mediascape: uses, ...  2010.0\n",
      "157  The Paradoxes of Media Globalization: On the B...  2010.0\n",
      "158  THE SHIFTING CROSS-MEDIA NEWS LANDSCAPE Challe...  2010.0\n",
      "159                                  CHATTING THE NEWS  2010.0\n",
      "160  The institutional logics of journalism: studyi...  2010.0\n",
      "\n",
      "[5157 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(alldf.head)\n",
    "text_y = alldf[[\"TI\", str(\"PY\")]]\n",
    "text_yd = text_y.drop_duplicates(keep='first', inplace=False)\n",
    "# print(len(text_yd))\n",
    "print(text_yd)\n",
    "# text_yd.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\yeartitle.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2009\n",
    "y2009 = text_yd[text_yd[\"PY\"] == 2009]\n",
    "title = y2009[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2009.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2010\n",
    "y2010 = text_yd[text_yd[\"PY\"] == 2010]\n",
    "title = y2010[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2010.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2011\n",
    "y2011 = text_yd[text_yd[\"PY\"] == 2011]\n",
    "title = y2011[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2011.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2012\n",
    "y2012 = text_yd[text_yd[\"PY\"] == 2012]\n",
    "title = y2012[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2012.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2013\n",
    "y2013 = text_yd[text_yd[\"PY\"] == 2013]\n",
    "title = y2013[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2013.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2014\n",
    "y2014 = text_yd[text_yd[\"PY\"] == 2014]\n",
    "title = y2014[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2014.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2015\n",
    "y2015 = text_yd[text_yd[\"PY\"] == 2015]\n",
    "title = y2015[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2015.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2016\n",
    "y2016 = text_yd[text_yd[\"PY\"] == 2016]\n",
    "title = y2016[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2016.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2017\n",
    "y2017 = text_yd[text_yd[\"PY\"] == 2017]\n",
    "title = y2017[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2017.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2018\n",
    "y2018 = text_yd[text_yd[\"PY\"] == 2018]\n",
    "title = y2018[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2018.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2019\n",
    "y2019 = text_yd[text_yd[\"PY\"] == 2019]\n",
    "title = y2019[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2019.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2020\n",
    "y2020 = text_yd[text_yd[\"PY\"] == 2020]\n",
    "title = y2020[\"TI\"].tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\y2020.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole news marketing data\n",
    "list1 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(1).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "# print(list1.head(n = 5))\n",
    "list2 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(2).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "# print(list2.head(n = 5))\n",
    "list3 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(3).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list4 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(4).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list5 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(5).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list6 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(6).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list7 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(7).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list8 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(8).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list9 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(9).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list10 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(10).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list11 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\savedrecs(11).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "frames = [list1, list2, list3, list4, list5, list6, list7, list8, list9, list10, list11]\n",
    "All_list = pd.concat(frames)\n",
    "# All_list.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\List\\\\all_list.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2014~2020 news marketing list\n",
    "list1 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Cited List\\\\savedrecs(1).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "# print(list1.head(n = 5))\n",
    "list2 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Cited List\\\\savedrecs(2).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "# print(list2.head(n = 5))\n",
    "list3 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Cited List\\\\savedrecs(3).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list4 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Cited List\\\\savedrecs(4).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list5 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Cited List\\\\savedrecs(5).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "list6 = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Cited List\\\\savedrecs(6).txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "frames = [list1, list2, list3, list4, list5, list6]\n",
    "All_list2 = pd.concat(frames)\n",
    "#All_list.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Cited List\\\\2014_2020_all_list.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_list2[\"ind\"] = range(len(All_list2))\n",
    "# print(All_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_list3 = All_list2[[\"AU\", \"ind\"]]\n",
    "print(All_list3)\n",
    "# All_list3.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Cited List\\\\2014_2020_all_list3.csv\", index = False)\n",
    "#delete more than 491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Cited List\\\\2014_2020_all_list3.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "AUN = news[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "AUN = AUN.reset_index(level = 1, drop = True).rename(\"AUN\")\n",
    "news = news.join(AUN)\n",
    "print(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for i in range(1,492):\n",
    "    AU = [] \n",
    "    AU = pd.DataFrame(AU)\n",
    "    df = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\Citing List\\\\savedrecs(\" + str(i) + \").txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "    df[\"ind\"] = i-1\n",
    "    AU = df[\"AU\"].str.split(\"; \",expand = True).stack()\n",
    "    AU = AU.reset_index(level = 1, drop = True).rename(\"AUC\")\n",
    "    df = df.join(AU)\n",
    "    frames.append(df)\n",
    "alldf = pd.concat(frames, sort = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf1 = alldf[[\"AU\", \"AUC\", \"TI\", \"RP\", \"PY\", \"ind\"]]\n",
    "print(alldf1)\n",
    "alldf1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\Cited List\\\\alldf1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(alldf)\n",
    "alldf2 = pd.merge(news, alldf1, how = \"left\", left_on= \"ind\", right_on= \"ind\")\n",
    "alldf3 = alldf2[[\"AUN\", \"AUC\"]]\n",
    "print(alldf2.head)\n",
    "alldf3.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\reviews.csv\", index = False)\n",
    "alldf2.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\allcolumn.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(alldf2.head)\n",
    "text = pd.read_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\Whole dataset_News Marketing\\\\2014_2020\\\\High self citation title.txt\", sep = \"\\t\", header = 0, index_col= False)\n",
    "text_y = text[\"TI\"]\n",
    "text_yd = text_y.drop_duplicates(keep='first', inplace=False)\n",
    "# print(len(text_yd))\n",
    "print(text_yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = text_yd.tolist()\n",
    "tokens = str(title).split()\n",
    "tokens = [tokens.strip(string.punctuation).lower() for tokens in tokens]\n",
    "tokens = [token.strip() for token in tokens if token.strip()!='']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words+=[\"cook\", \"cooking\",\"cookbook\",\"cooker\",\"book\",\"recipes\",\"food\", \"doi\"]\n",
    "stop_words += ['•','’','“','--','–',\"''\",'—','”','``']\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "filtered_dict = {word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "sorted_dict = sorted(filtered_dict.items(), key = lambda x: x[1], reverse = True)\n",
    "# print(sorted_dict)\n",
    "sorted_dict1 = pd.DataFrame(sorted_dict)\n",
    "sorted_dict1.to_csv(\"C:\\\\ZLIU\\\\SIT\\\\686\\\\citing.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
